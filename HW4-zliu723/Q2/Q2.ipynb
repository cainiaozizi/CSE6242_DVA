{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np  # http://www.numpy.org\n",
    "import ast\n",
    "from datetime import datetime\n",
    "from math import log, floor, ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the Utility class's methods. You can also add additional methods as required but don't change existing methods' arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility(object):\n",
    "    \n",
    "    # This method computes entropy for information gain\n",
    "    def entropy(self, class_y):\n",
    "        entropy = 0\n",
    "        number_of_label = len(class_y) \n",
    "        if number_of_label <= 1:\n",
    "            return 0\n",
    "        \n",
    "        value,counts = np.unique(class_y, return_counts = True)\n",
    "      \n",
    "        probs = counts/number_of_label        \n",
    "        number_of_class = np.count_nonzero(probs)\n",
    "        \n",
    "        if number_of_class <= 1:\n",
    "            return 0\n",
    "        \n",
    "        for i in probs:\n",
    "            entropy -= i*log(i,2) \n",
    "        \n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def partition_classes(self, X, y, split_attribute, split_val):\n",
    "        X_left = []\n",
    "        X_right = []\n",
    "        y_left = []\n",
    "        y_right = []\n",
    "      \n",
    "        for i in range(len(X)):\n",
    "            x = X[i]\n",
    "            if x[split_attribute] <= split_val:\n",
    "                X_left.append(x)\n",
    "                y_left.append(y[i])\n",
    "            else:\n",
    "                X_right.append(x)\n",
    "                y_right.append(y[i])\n",
    "\n",
    "#         print(\"X_left\",*X_left,sep='\\n')\n",
    "#         print(\"X_right\",*X_right,sep='\\n')\n",
    "#         print(\"y_left\",y_left,sep='\\n')\n",
    "#         print(\"y_right\",y_right,sep='\\n')\n",
    "        \n",
    "        return (X_left, X_right, y_left, y_right)\n",
    "    \n",
    "    def information_gain(self, previous_y, current_y):\n",
    "        # Inputs:\n",
    "        #   previous_y: the distribution of original labels (0's and 1's)\n",
    "        #   current_y:  the distribution of labels after splitting based on a particular\n",
    "        #               split attribute and split value\n",
    "\n",
    "        # TODO: Compute and return the information gain from partitioning the previous_y labels\n",
    "        # into the current_y labels.\n",
    "        # You will need to use the entropy function above to compute information gain\n",
    "        # Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "\n",
    "        \"\"\"\n",
    "        Example:\n",
    "\n",
    "        previous_y = [0,0,0,1,1,1]\n",
    "        current_y = [[0,0], [1,1,1,0]]\n",
    "\n",
    "        info_gain = 0.45915\n",
    "        \"\"\"\n",
    "        info_gain = self.entropy(previous_y)\n",
    "        for i in current_y:\n",
    "            info_gain -= self.entropy(i)*len(i)/len(previous_y)\n",
    "        \n",
    "        return info_gain\n",
    "\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        # Inputs:\n",
    "        #   X       : Data containing all attributes\n",
    "        #   y       : labels\n",
    "        # TODO: For each node find the best split criteria and return the \n",
    "        # split attribute, spliting value along with \n",
    "        # X_left, X_right, y_left, y_right (using partition_classes)\n",
    "        '''\n",
    "\n",
    "        NOTE: Just like taught in class, don't use all the features for a node.\n",
    "        Repeat the steps:\n",
    "\n",
    "        1. Select m attributes out of d available attributes\n",
    "        2. Pick the best variable/split-point among the m attributes\n",
    "        3. return the split attributes, split point, left and right children nodes data \n",
    "\n",
    "        '''\n",
    "        X_left, X_right, y_left, y_right = [], [], [], []\n",
    "\n",
    "        split_attribute = 0\n",
    "        split_value = 0\n",
    "        current_info_gain = 0\n",
    "\n",
    "        for index in range(0,len(X[0])):\n",
    "            #create an array for all values for each attribute\n",
    "            m = np.asarray(X)[:,index]\n",
    "            split_val = np.asarray(np.mean([i for i in m]))\n",
    "\n",
    "            X_left, X_right, y_left, y_right = self.partition_classes(X,y, index, split_val)\n",
    "\n",
    "            info_gain = self.information_gain(y,[y_left, y_right])\n",
    "\n",
    "            if info_gain > current_info_gain:\n",
    "                current_info_gain = info_gain\n",
    "                split_value = split_val\n",
    "                split_attribute = index\n",
    "        \n",
    "        X_left, X_right, y_left, y_right = self.partition_classes(X, y, split_attribute, split_value)\n",
    "        \n",
    "#         print('current_info_gain:',current_info_gain)\n",
    "#         print('split_value:',split_value)\n",
    "#         print('split_attribute:',split_attribute)\n",
    "#         print('X_left:',X_left)\n",
    "#         print('X_right:',X_right)\n",
    "#         print('y_left:',y_left)\n",
    "#         print('y_right:',y_right)            \n",
    "        return split_attribute,split_value,X_left, X_right,y_left,y_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the classes 'DecisionTree' and 'RandomForest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please modify the 'DecisionTree' and 'RandomForest' classes below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(Utility):\n",
    "    def __init__(self, max_depth):\n",
    "        # Initializing the tree as an empty dictionary or list, as preferred\n",
    "        self.tree = [-1,0,np.nan,np.nan] #set up tree structure: [split_attribute index, split_value, left, right]\n",
    "        self.leaf = 3\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def build_tree(self,X,y,depth =0):\n",
    "        #set up termination condition\n",
    "        if depth > self.max_depth or len(set(y)) == 1 or len(y) <= self.leaf:\n",
    "            return [-1,np.mean(y),np.nan,np.nan]\n",
    "        #create the node\n",
    "        split_attribute, split_value, XLeft,XRight, yLeft, yRight= self.best_split(X, y)\n",
    "        #call the recusive function to build the tree\n",
    "        left = self.build_tree(XLeft,yLeft,depth +1)\n",
    "        right = self.build_tree(XRight,yRight,depth +1)\n",
    "        \n",
    "#         print(split_attribute, split_value, left, right)\n",
    "        return [split_attribute, split_value, left, right]\n",
    "    \n",
    "    def learn(self, X, y, depth=0):\n",
    "        # TODO: Train the decision tree (self.tree) using the the sample X and labels y\n",
    "        # You will have to make use of the functions in Utility class to train the tree\n",
    "\n",
    "        # Use the function best_split in Utility class to get the best split and \n",
    "        # data corresponding to left and right child nodes\n",
    "        \n",
    "        # One possible way of implementing the tree:\n",
    "        #    Each node in self.tree could be in the form of a dictionary:\n",
    "        #       https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        #    For example, a non-leaf node with two children can have a 'left' key and  a \n",
    "        #    'right' key. You can add more keys which might help in classification\n",
    "        #    (eg. split attribute and split value)\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        self.tree = self.build_tree(X,y,0)\n",
    "        #############################################\n",
    "        \n",
    "    def classify_temp(self, record,tree):\n",
    "\n",
    "        split_attribute = tree[0]\n",
    "        split_value = tree[1]\n",
    "        left = tree[2]\n",
    "        right = tree[3]\n",
    "#         print(split_attribute, split_value, left, right)\n",
    "        \n",
    "        #the termininate condiction\n",
    "        if split_attribute == -1:\n",
    "            return split_value\n",
    "        \n",
    "        if record[split_attribute] <= split_value:\n",
    "            return self.classify_temp(record,left)\n",
    "        else:\n",
    "            return self.classify_temp(record,right)\n",
    "\n",
    "    def classify(self, record):\n",
    "        # TODO: classify the record using self.tree and return the predicted label\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        return self.classify_temp(record,self.tree)\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    num_trees = 0\n",
    "    decision_trees = []\n",
    "\n",
    "    # the bootstrapping datasets for trees\n",
    "    # bootstraps_datasets is a list of lists, where each list in bootstraps_datasets is a bootstrapped dataset.\n",
    "    bootstraps_datasets = []\n",
    "\n",
    "    # the true class labels, corresponding to records in the bootstrapping datasets\n",
    "    # bootstraps_labels is a list of lists, where the 'i'th list contains the labels corresponding to records in\n",
    "    # the 'i'th bootstrapped dataset.\n",
    "    bootstraps_labels = []\n",
    "\n",
    "    def __init__(self, num_trees):\n",
    "        # Initialization done here\n",
    "        np.random.seed(10)\n",
    "        self.num_trees = num_trees\n",
    "        self.decision_trees = [DecisionTree(max_depth=50) for i in range(num_trees)]\n",
    "        self.bootstraps_datasets = []\n",
    "        self.bootstraps_labels = []\n",
    "\n",
    "    def _bootstrapping(self, XX, n):\n",
    "        # Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        #\n",
    "        # TODO: Create a sample dataset of size n by sampling with replacement\n",
    "        #       from the original dataset XX.\n",
    "        # Note that you would also need to record the corresponding class labels\n",
    "        # for the sampled records for training purposes.\n",
    "\n",
    "        samples = []  # sampled dataset\n",
    "        labels = []  # class labels for the sampled records\n",
    "        for i in range(n):\n",
    "            index = np.random.randint(len(XX))\n",
    "            samples.append(XX[index][:-1])\n",
    "            labels.append(XX[index][-1])\n",
    "        return (samples, labels)\n",
    "\n",
    "    def bootstrapping(self, XX):\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.num_trees):\n",
    "            data_sample, data_label = self._bootstrapping(XX, len(XX))\n",
    "            self.bootstraps_datasets.append(data_sample)\n",
    "            self.bootstraps_labels.append(data_label)\n",
    "\n",
    "    def fitting(self):\n",
    "        # TODO: Train `num_trees` decision trees using the bootstraps datasets\n",
    "        # and labels by calling the learn function from your DecisionTree class.\n",
    "        for i in range(self.num_trees):\n",
    "            X = self.bootstraps_datasets[i]\n",
    "            y = self.bootstraps_labels[i]\n",
    "            self.decision_trees[i].learn(X,y)\n",
    "\n",
    "    def voting(self, X):\n",
    "        y = []\n",
    "\n",
    "        for record in X:\n",
    "            # Following steps have been performed here:\n",
    "            #   1. Find the set of trees that consider the record as an\n",
    "            #      out-of-bag sample.\n",
    "            #   2. Predict the label using each of the above found trees.\n",
    "            #   3. Use majority vote to find the final label for this recod.\n",
    "            votes = []\n",
    "            for i in range(len(self.bootstraps_datasets)):\n",
    "                dataset = self.bootstraps_datasets[i]\n",
    "                if record not in dataset:\n",
    "                    OOB_tree = self.decision_trees[i]\n",
    "                    effective_vote = OOB_tree.classify(record)\n",
    "                    votes.append(effective_vote)\n",
    "\n",
    "            counts = np.bincount(votes)\n",
    "\n",
    "            \n",
    "            if len(counts) == 0:\n",
    "                # TODO: Special case\n",
    "                #  Handle the case where the record is not an out-of-bag sample\n",
    "                #  for any of the trees.\n",
    "                \n",
    "                ind = -1\n",
    "                for index in range(len(self.bootstraps_datasets)):\n",
    "                    ind = self.bootstraps_datasets[index].index(record)\n",
    "                    if ind != -1:\n",
    "                        y = np.append(y, self.bootstraps_labels[index][ind])\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                y = np.append(y, np.argmax(counts))\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def user(self):\n",
    "        \"\"\"\n",
    "        :return: string\n",
    "        your GTUsername, NOT your 9-Digit GTId  \n",
    "        \"\"\"\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        return 'Zliu723'\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Initialize according to your implementation\n",
    "# VERY IMPORTANT: Minimum forest_size should be 10\n",
    "forest_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not modify the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading the data\n",
      "__Name: Zliu723__\n",
      "creating the bootstrap datasets\n",
      "fitting the forest\n",
      "accuracy: 0.7366\n",
      "OOB estimate: 0.2634\n",
      "Execution time: 0:00:07.125732\n"
     ]
    }
   ],
   "source": [
    "# start time \n",
    "start = datetime.now()\n",
    "X = list()\n",
    "y = list()\n",
    "XX = list()  # Contains data features and data labels\n",
    "numerical_cols = set([i for i in range(0, 9)])  # indices of numeric attributes (columns)\n",
    "\n",
    "# Loading data set\n",
    "print(\"reading the data\")\n",
    "with open(\"pima-indians-diabetes.csv\") as f:\n",
    "    next(f, None)\n",
    "    for line in csv.reader(f, delimiter=\",\"):\n",
    "        xline = []\n",
    "        for i in range(len(line)):\n",
    "            if i in numerical_cols:\n",
    "                xline.append(ast.literal_eval(line[i]))\n",
    "            else:\n",
    "                xline.append(line[i])\n",
    "\n",
    "        X.append(xline[:-1])\n",
    "        y.append(xline[-1])\n",
    "        XX.append(xline[:])\n",
    "\n",
    "# Initializing a random forest.\n",
    "randomForest = RandomForest(forest_size)\n",
    "\n",
    "# printing the name\n",
    "print(\"__Name: \" + randomForest.user()+\"__\")\n",
    "\n",
    "# Creating the bootstrapping datasets\n",
    "print(\"creating the bootstrap datasets\")\n",
    "randomForest.bootstrapping(XX)\n",
    "\n",
    "# Building trees in the forest\n",
    "print(\"fitting the forest\")\n",
    "randomForest.fitting()\n",
    "\n",
    "# Calculating an unbiased error estimation of the random forest\n",
    "# based on out-of-bag (OOB) error estimate.\n",
    "y_predicted = randomForest.voting(X)\n",
    "\n",
    "# Comparing predicted and true labels\n",
    "results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "# Accuracy\n",
    "accuracy = float(results.count(True)) / float(len(results))\n",
    "\n",
    "print(\"accuracy: %.4f\" % accuracy)\n",
    "print(\"OOB estimate: %.4f\" % (1 - accuracy))\n",
    "\n",
    "# end time\n",
    "print(\"Execution time: \" + str(datetime.now() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
